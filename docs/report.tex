\documentclass[11pt,a4paper]{article}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[ruled,vlined,linesnumbered,algosection,algo2e]{algorithm2e}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.0cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{amsmath}
\titlelabel{\thetitle.\quad}

\pagestyle{fancy}
\fancyhf{}
\rhead{Data Structures and Algorithms}
\lhead{Lab 3: Sorting}
\rfoot{Page \thepage}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\title{
\normalfont \LARGE
\textsc{University of Science} \\ [25pt] % Your university, school and/or department name(s)
\horrule{1pt} \\[0.4cm] % Thin top horizontal rule
\huge Sorting Algorithms - An Overview \\ % The assignment title
\horrule{1pt} \\[0.6cm] % Thick bottom horizontal rule adjust 0.6cm suitably
\Large Data Structures and Algorithms\\[0.5cm]  %% sub title, adjust \Large
}

\author{Huynh Minh Tuan - 20120024@student.hcmus.edu.vn}
\date{November 2021}

\begin{document}

\maketitle

\begin{abstract}
    Sorting is nothing but alphabetizing, categorizing, arranging, or putting items in an ordered sequence. 
    It is a key fundamental operation in the field of computer science. It is of extreme importance because it adds usefulness to data.
    In this report, I have compared eleven common sorting algorithms (Selection Sort, Insertion Sort, Bubble Sort, Shaker Sort, Shell Sort, Heap
    Sort, Merge Sort, Quick Sort, Counting Sort, Radix Sort, and Flash Sort). I have developed a program in C++, Python and experimented with several input sizes
    10,000, 30,000, 50,000, 100,000, 300,000, and 500,000 elements. The performance and efficiency of these algorithms in terms of CPU time consumption 
    as well as the number of comparisons that have been recorded and presented in tabular and graphical form.
\end{abstract}

\section{Introduction}
\tab Sorting is not a leap but it has emerged in parallel with the development of the human mind.
In computer science, alphabetizing, arranging, categorizing, or putting data items in an ordered sequence on the basis of similar properties is called sorting.
Sorting is of key importance because it optimizes the usefulness of data.
We can observe plenty of sorting examples in our daily life, e.g. we can easily find required items in a shopping mall or utility store because the items are kept categorically.
\newline
\newline
\tab The items to be sorted may be in various forms i.e. random as a whole, already sorted, very small or extremely large in numer, sorted in reverse order etc.
There is no algorithm that is best for sorting all types of data. 
We must be familiar with sorting algorithms in terms of their suitability in a particular situation.
\newline
\newline
\tab In this paper, I am going to compare eleven common sorting algorithms (Selection Sort, Insertion Sort, Bubble Sort, Shaker Sort, Shell Sort, Heap
Sort, Merge Sort, Quick Sort, Counting Sort, Radix Sort, and Flash Sort) for their CPU time consumption and number of compared operations on four different data arrangements 
(Sorted data (in ascending order), Nearly sorted data, Revherse sorted data, and Randomized data).


\section{Algorithm presentation}
\subsection{Selection Sort}

\subsubsection*{Idea}
\tab The Selection Sort is based on the idea of finding the minimum element in an unsorted array and then putting it in its correct position in a sorted array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \For{$i \gets 1$ to $N$}{
    $minIndex \gets i$\\
    \For{$j \gets i+1$ to $N$}{
        \If{$a_{minIndex} > a_{j}$}{
            $minIndex \gets j$
        }
    }
    swap($a_{minIndex}$, $a_{i}$)
  }
  \caption{Selection Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N^2)$ \\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Insertion Sort}
\subsubsection*{Idea}
\tab The main idea of insertion sort is that array is divided in two parts which left part is already sorted, and right part is unsorted.
Values from the unsorted part are picked and placed at the correct position in the sorted part.
So, at every iteration sorted part grows by one element which is called key.
During an iteration, if compared element is greater than key then compared element has to shift to right to open a position for key.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \For{$i \gets 2$ to $N$}{
    $k \gets i-1$\\
    $key \gets a_i$\\
    \While{$a_k > key$ and $k \geq 0$}{
      $a_{k+1} \gets a_{k}$\\
      $k \gets k-1$
    }
    $a_{k+1} \gets key$
  }
  \caption{Insertion Sort}
\end{algorithm2e}
\newpage
\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N^2)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Bubble Sort}
\subsubsection*{Idea}
\tab Bubble sort is based on the idea of repeatedly comparing pairs of adjacent elements and then 
swapping their positions if they exist in the wrong order.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \For{$i \gets N$ to $1$}{
    $isSwap \gets False$\\
    \For{$j \gets 1$ to $i-1$}{
        \If{$a_{j} > a_{j+1}$}{
          $isSwap \gets True$\\
          swap($a_j$, $a_{j+1}$)
        }
    }
    \If{$isSwap = False$}{
      stop algorithm
    }
  }
  \caption{Bubble Sort}
\end{algorithm2e}

In this paper, I implemented bubble sort with a flag $isSwap$ to stop the algorithm early
when the array is sorted.

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N^2)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Shaker Sort}
\subsubsection*{Idea}
\tab Shaker sort is a bidirectional version of bubble sort.
The Bubble sort algorithm always traverses elements from left and moves the largest 
element to its correct position in first iteration and second largest in second iteration and so on.
Shaker sort orders the array in both directions. Hence every iteration of the algorithm consists of two phases. 
In the first one, the lightest bubble ascends to the end of the array, in the second phase the heaviest bubble descends to the beginning of the array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  $left \gets 0$ \\
  $right \gets N-1$ \\
  $k \gets 0$\\
  \For{$i \gets left$ to $right$}{
    \tcp{phase 1}
    $isSwap \gets False$\\
    \For{$j \gets left$ to $right-1$}{
        \If{$a_{j} > a_{j+1}$}{
          $isSwap \gets True$\\
          swap($a_j$, $a_{j+1}$)\\
          $k \gets j$
        }
    }
    \If{$isSwap = False$}{
      stop algorithm
    }
    $right \gets k$

    \tcp{phase 2}
    $isSwap \gets False$\\
    \For{$j \gets right$ to $left+1$}{
        \If{$a_{j} < a_{j-1}$}{
          $isSwap \gets True$\\
          swap($a_j$, $a_{j-1}$)\\
          $k \gets j$
        }
    }
    \If{$isSwap = False$}{
      stop algorithm
    }
    $left \gets k$
  }
  \caption{Shaker Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N^2)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Shell Sort}
\subsubsection*{Idea}
Shell sort is a generalized version of the insertion sort algorithm. 
It first sorts elements that are far apart from each other and successively reduces the interval between the elements to be sorted.

The interval between the elements is reduced based on the sequence used. 
Some of the optimal sequences that can be used in the shell sort algorithm are:
\begin{itemize}
\item Shell's original sequence
\item Knuth's increments
\item Sedgewick's increments
\item Hibbard's increments
\item ...
\end{itemize}

In this paper, I only implemented the algorithm with optimal sequence based on Shell's original sequence.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  $interval \gets \dfrac{N}{2}$\\
  \While{$interval > 0$}{
    \For{$i \gets interval$ to $N$}{
      $temp \gets a_i$\\
      $j \gets i$\\
      \While{$interval \leq j$ and $a_{j-interval} > temp$}{
        $a_j \gets a_{j-interval}$\\
        $j \gets j - interval$
      }
    }
    $a_j \gets temp$\\
    $interval \gets \dfrac{interval}{2}$
  }
  \caption{Shell Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N\log N)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Heap Sort}
\subsubsection*{Idea}
\tab Heap sort is a comparison-based sorting algorithm. 
Heap sort can be thought of as an improved selection sort: like selection sort, heap sort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. 
\newline
\tab Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region 
in a \textbf{heap data structure} to more quickly find the largest element in each step.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \SetKwFunction{FMain}{HeapRebuild}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $pos$, $N$}}{
    \While{$2\cdot pos + 1 <= N$}{
      $j = 2\cdot pos + 1$\\
      \If{$j < N$}{
        \If{$a_j < a_{j+1}$}{
          $j \gets j + 1$
        }
      }
      \If{$a_{pos} \geq a_j$}{
        \textbf{return}
      }
      swap($a_{pos}$, $a_j$)\\
      $pos \gets j$
    }
  }

  \SetKwFunction{FMain}{HeapConstruct}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $N$}}{
    \For{$i \gets N/2$ to $0$}{
      HeapRebuild($a$, $i$, $n$)
    }
  }

  \SetKwFunction{FMain}{HeapSort}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $N$}}{
    HeapConstruct(a, N)\\
    $r \gets N$\\
    \While{$r > 0$}{
      swap($a_1$, $a_N$)\\
      HeapRebuild(a, 1, r)\\
      $r \gets r-1$
    }
  }
  \caption{Heap Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N \log N)$ \\
Average case time complexity: $O(N\log N)$\\
Worst case time complexity: $O(N \log N)$ \\
Worst case space complexity: $O(1)$

\subsection{Merge Sort}
\subsubsection*{Idea}
\tab Merge sort is a recursive sorting algorithm based on a "divide and conquer" approach.
It divides the input array into two halves, calls itself for the two halves, and then merges the two sorted halves.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \SetKwFunction{FMain}{Merge}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $first$, $mid$, $last$}}{
    $n_1 \gets mid - first + 1$\\
    $n_2 \gets last - mid$\\
    $L \gets a_{first}, a_{first+1}, ..., a_{mid}$\\
    $R \gets a_{mid+1}, a_{mid+2}, ..., a_{last}$\\
    \tcp{merge}
    $i \gets 0$\\
    $j \gets 0$\\
    $k \gets first$\\
    \While{$i < n_1$ and $j < n_2$}{
      \eIf{$L_i < R_j$}{
        $a_k \gets L_i$\\
        $i \gets i+1$
      }{
        $a_k \gets R_j$\\
        $j \gets j+1$
      }
      $k \gets k+1$
    }
    \While{$j < n_2$}{
      $a_k \gets R_j$\\
      $k \gets k+1$\\
      $j \gets j+1$
    }
    \While{$i < n_1$}{
      $a_k \gets L_i$\\
      $k \gets k+1$\\
      $i \gets i+1$
    }
  }
  \SetKwFunction{FMain}{MergeSort}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $first$, $last$}}{
    \If{$first < last$}{
      $mid \gets first + (last - first)/2$\\
      MergeSort($a$, $first$, $mid$)\\
      MergeSort($a$, $mid+1$, $last$)\\
      Merge($a$, $first$, $mid$, $last$)
    }
  }
  \caption{Merge Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N \log N)$ \\
Average case time complexity: $O(N\log N)$\\
Worst case time complexity: $O(N \log N)$ \\
Worst case space complexity: $O(N)$

\subsection{Quick Sort}
\subsubsection*{Idea}
Like Merge Sort, Quick Sort is a Divide and Conquer algorithm. 
It picks an element as a pivot and partitions the given array around the picked pivot. 
There are many different versions of quickSort that pick pivot in different ways. 

\begin{itemize}
  \item Pick first element as pivot.
  \item Pick last element as pivot
  \item Pick a random element as pivot.
  \item Pick median as pivot.
\end{itemize}

In this paper, I implemented the algorithm with pivot is a median of array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \SetKwFunction{FMain}{Partition}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $l$, $r$}}{
    $pivot \gets a_{(l+r)/2}$\\
    \While{$l \leq r$}{
      \While{$a_l < pivot$}{
        $l \gets l+1$
      }
      \While{$a_r > pivot$}{
        $r \gets r-1$
      }
      \If{$l \leq r$}{
        swap($a_l$, $a_r$)\\
        $l \gets l+1$\\
        $r \gets r-1$
      }
    }
    \textbf{return} l
  }
  \SetKwFunction{FMain}{QuickSort}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $l$, $r$}}{
    \If{$l < r$}{
      $mid \gets$ Partition($a$, $l$, $r$)\\
      QuickSort($a$, $l$, $mid-1$)\\
      QuickSort($a$, $mid$, $r$)
    }
  }
  \caption{Quick Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N \log N)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Counting Sort}
\subsection{Radix Sort}
\subsection{Flash Sort}

\end{document}