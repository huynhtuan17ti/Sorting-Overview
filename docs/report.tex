\documentclass[11pt,a4paper]{article}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[ruled,vlined,linesnumbered,algosection,algo2e]{algorithm2e}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage[left=2.00cm, right=2.00cm, top=2.00cm, bottom=2.0cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{float}
\usepackage{hyperref}

\newenvironment{fignote}{\begin{quote}\footnotesize}{\end{quote}}
\titlelabel{\thetitle.\quad}

\pagestyle{fancy}
\fancyhf{}
\rhead{Data Structures and Algorithms}
\lhead{Lab 3: Sorting}
\rfoot{Page \thepage}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}
\title{
\normalfont \LARGE
\textsc{University of Science} \\ [25pt] % Your university, school and/or department name(s)
\horrule{1pt} \\[0.4cm] % Thin top horizontal rule
\huge Sorting Algorithms - An Overview \\ % The assignment title
\horrule{1pt} \\[0.6cm] % Thick bottom horizontal rule adjust 0.6cm suitably
\Large Data Structures and Algorithms\\[0.5cm]  %% sub title, adjust \Large
}

\author{Huynh Minh Tuan - 20120024@student.hcmus.edu.vn}
\date{November 2021}

\begin{document}

\maketitle

\begin{abstract}
    Sorting is nothing but alphabetizing, categorizing, arranging, or putting items in an ordered sequence. 
    It is a key fundamental operation in the field of computer science. It is of extreme importance because it adds usefulness to data.
    In this report, I have compared eleven common sorting algorithms (Selection Sort, Insertion Sort, Bubble Sort, Shaker Sort, Shell Sort, Heap
    Sort, Merge Sort, Quick Sort, Counting Sort, Radix Sort, and Flash Sort). I have developed a program in C++, Python and experimented with several input sizes
    10,000, 30,000, 50,000, 100,000, 300,000, and 500,000 elements. The performance and efficiency of these algorithms in terms of CPU time consumption 
    as well as the number of comparisons that have been recorded and presented in tabular and graphical form.
\end{abstract}

\section{Introduction}
\tab Sorting is not a leap but it has emerged in parallel with the development of the human mind.
In computer science, alphabetizing, arranging, categorizing, or putting data items in an ordered sequence on the basis of similar properties is called sorting.
Sorting is of key importance because it optimizes the usefulness of data.
We can observe plenty of sorting examples in our daily life, e.g. we can easily find required items in a shopping mall or utility store because the items are kept categorically.
\newline
\newline
\tab The items to be sorted may be in various forms i.e. random as a whole, already sorted, very small or extremely large in numer, sorted in reverse order etc.
There is no algorithm that is best for sorting all types of data. 
We must be familiar with sorting algorithms in terms of their suitability in a particular situation.
\newline
\newline
\tab In this paper, I am going to compare eleven common sorting algorithms (Selection Sort, Insertion Sort, Bubble Sort, Shaker Sort, Shell Sort, Heap
Sort, Merge Sort, Quick Sort, Counting Sort, Radix Sort, and Flash Sort) for their CPU time consumption and number of compared operations on four different data arrangements 
(Sorted data (in ascending order), Nearly sorted data, Revherse sorted data, and Randomized data).


\section{Algorithm presentation}
\subsection{Selection Sort}

\subsubsection*{Idea}
The Selection Sort is based on the idea of finding the minimum element in an unsorted array and then putting it in its correct position in a sorted array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \For{$i \gets 1$ to $N$}{
    $minIndex \gets i$\\
    \For{$j \gets i+1$ to $N$}{
        \If{$a_{minIndex} > a_{j}$}{
            $minIndex \gets j$
        }
    }
    swap($a_{minIndex}$, $a_{i}$)
  }
  \caption{Selection Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N^2)$ \\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Insertion Sort}
\subsubsection*{Idea}
The main idea of insertion sort is that array is divided in two parts which left part is already sorted, and right part is unsorted.
Values from the unsorted part are picked and placed at the correct position in the sorted part.
So, at every iteration sorted part grows by one element which is called key.
During an iteration, if compared element is greater than key then compared element has to shift to right to open a position for key.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \For{$i \gets 2$ to $N$}{
    $k \gets i-1$\\
    $key \gets a_i$\\
    \While{$a_k > key$ and $k \geq 0$}{
      $a_{k+1} \gets a_{k}$\\
      $k \gets k-1$
    }
    $a_{k+1} \gets key$
  }
  \caption{Insertion Sort}
\end{algorithm2e}
\newpage
\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N^2)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Bubble Sort}
\subsubsection*{Idea}
Bubble sort is based on the idea of repeatedly comparing pairs of adjacent elements and then 
swapping their positions if they exist in the wrong order.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \For{$i \gets N$ to $1$}{
    $isSwap \gets False$\\
    \For{$j \gets 1$ to $i-1$}{
        \If{$a_{j} > a_{j+1}$}{
          $isSwap \gets True$\\
          swap($a_j$, $a_{j+1}$)
        }
    }
    \If{$isSwap = False$}{
      stop algorithm
    }
  }
  \caption{Bubble Sort}
\end{algorithm2e}

In this paper, I implemented bubble sort with a flag $isSwap$ to stop the algorithm early
when the array is sorted.

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N^2)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Shaker Sort}
\subsubsection*{Idea}
Shaker sort is a bidirectional version of bubble sort.
The Bubble sort algorithm always traverses elements from left and moves the largest 
element to its correct position in first iteration and second largest in second iteration and so on.
Shaker sort orders the array in both directions. Hence every iteration of the algorithm consists of two phases. 
In the first one, the lightest bubble ascends to the end of the array, in the second phase the heaviest bubble descends to the beginning of the array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  $left \gets 0$ \\
  $right \gets N-1$ \\
  $k \gets 0$\\
  \For{$i \gets left$ to $right$}{
    \tcp{phase 1}
    $isSwap \gets False$\\
    \For{$j \gets left$ to $right-1$}{
        \If{$a_{j} > a_{j+1}$}{
          $isSwap \gets True$\\
          swap($a_j$, $a_{j+1}$)\\
          $k \gets j$
        }
    }
    \If{$isSwap = False$}{
      stop algorithm
    }
    $right \gets k$

    \tcp{phase 2}
    $isSwap \gets False$\\
    \For{$j \gets right$ to $left+1$}{
        \If{$a_{j} < a_{j-1}$}{
          $isSwap \gets True$\\
          swap($a_j$, $a_{j-1}$)\\
          $k \gets j$
        }
    }
    \If{$isSwap = False$}{
      stop algorithm
    }
    $left \gets k$
  }
  \caption{Shaker Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N^2)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Shell Sort}
\subsubsection*{Idea}
Shell sort is a generalized version of the insertion sort algorithm. 
It first sorts elements that are far apart from each other and successively reduces the interval between the elements to be sorted.

The interval between the elements is reduced based on the sequence used. 
Some of the optimal sequences that can be used in the shell sort algorithm are:
\begin{itemize}
\item Shell's original sequence
\item Knuth's increments
\item Sedgewick's increments
\item Hibbard's increments
\item ...
\end{itemize}

In this paper, I only implemented the algorithm with optimal sequence based on Shell's original sequence.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  $interval \gets \dfrac{N}{2}$\\
  \While{$interval > 0$}{
    \For{$i \gets interval$ to $N$}{
      $temp \gets a_i$\\
      $j \gets i$\\
      \While{$interval \leq j$ and $a_{j-interval} > temp$}{
        $a_j \gets a_{j-interval}$\\
        $j \gets j - interval$
      }
    }
    $a_j \gets temp$\\
    $interval \gets \dfrac{interval}{2}$
  }
  \caption{Shell Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N\log N)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Heap Sort}
\subsubsection*{Idea}
Heap sort is a comparison-based sorting algorithm. 
Heap sort can be thought of as an improved selection sort: like selection sort, heap sort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. 
\newline
\tab Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region 
in a \textbf{heap data structure} to more quickly find the largest element in each step.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \SetKwFunction{FMain}{HeapRebuild}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $pos$, $N$}}{
    \While{$2\cdot pos + 1 <= N$}{
      $j = 2\cdot pos + 1$\\
      \If{$j < N$}{
        \If{$a_j < a_{j+1}$}{
          $j \gets j + 1$
        }
      }
      \If{$a_{pos} \geq a_j$}{
        \textbf{return}
      }
      swap($a_{pos}$, $a_j$)\\
      $pos \gets j$
    }
  }

  \SetKwFunction{FMain}{HeapConstruct}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $N$}}{
    \For{$i \gets N/2$ to $0$}{
      \Call{HeapRebuild}{$a$, $i$, $n$}
    }
  }

  \SetKwFunction{FMain}{HeapSort}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $N$}}{
    \Call{HeapConstruct}{a, N}\\
    $r \gets N$\\
    \While{$r > 0$}{
      swap($a_1$, $a_N$)\\
      \Call{HeapRebuild}{a, 1, r}\\
      $r \gets r-1$
    }
  }
  \caption{Heap Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N \log N)$ \\
Average case time complexity: $O(N\log N)$\\
Worst case time complexity: $O(N \log N)$ \\
Worst case space complexity: $O(1)$

\subsection{Merge Sort}
\subsubsection*{Idea}
Merge sort is a recursive sorting algorithm based on a "divide and conquer" approach.
It divides the input array into two halves, calls itself for the two halves, and then merges the two sorted halves.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \SetKwFunction{FMain}{Merge}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $first$, $mid$, $last$}}{
    $n_1 \gets mid - first + 1$\\
    $n_2 \gets last - mid$\\
    $L \gets a_{first}, a_{first+1}, ..., a_{mid}$\\
    $R \gets a_{mid+1}, a_{mid+2}, ..., a_{last}$\\
    \tcp{merge}
    $i \gets 0$\\
    $j \gets 0$\\
    $k \gets first$\\
    \While{$i < n_1$ and $j < n_2$}{
      \eIf{$L_i < R_j$}{
        $a_k \gets L_i$\\
        $i \gets i+1$
      }{
        $a_k \gets R_j$\\
        $j \gets j+1$
      }
      $k \gets k+1$
    }
    \While{$j < n_2$}{
      $a_k \gets R_j$\\
      $k \gets k+1$\\
      $j \gets j+1$
    }
    \While{$i < n_1$}{
      $a_k \gets L_i$\\
      $k \gets k+1$\\
      $i \gets i+1$
    }
  }
  \SetKwFunction{FMain}{MergeSort}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $first$, $last$}}{
    \If{$first < last$}{
      $mid \gets first + (last - first)/2$\\
      \Call{MergeSort}{$a$, $first$, $mid$}\\
      \Call{MergeSort}{$a$, $mid+1$, $last$}\\
      \Call{Merge}{$a$, $first$, $mid$, $last$}
    }
  }
  \caption{Merge Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N \log N)$ \\
Average case time complexity: $O(N\log N)$\\
Worst case time complexity: $O(N \log N)$ \\
Worst case space complexity: $O(N)$

\subsection{Quick Sort}
\subsubsection*{Idea}
Like Merge Sort, Quick Sort is a Divide and Conquer algorithm. 
It picks an element as a pivot and partitions the given array around the picked pivot. 
There are many different versions of quickSort that pick pivot in different ways. 

\begin{itemize}
  \item Pick first element as pivot.
  \item Pick last element as pivot
  \item Pick a random element as pivot.
  \item Pick median as pivot.
\end{itemize}

In this paper, I implemented the algorithm with pivot is a median of array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \SetKwFunction{FMain}{Partition}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $l$, $r$}}{
    $pivot \gets a_{(l+r)/2}$\\
    \While{$l \leq r$}{
      \While{$a_l < pivot$}{
        $l \gets l+1$
      }
      \While{$a_r > pivot$}{
        $r \gets r-1$
      }
      \If{$l \leq r$}{
        swap($a_l$, $a_r$)\\
        $l \gets l+1$\\
        $r \gets r-1$
      }
    }
    \textbf{return} l
  }
  \SetKwFunction{FMain}{QuickSort}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $l$, $r$}}{
    \If{$l < r$}{
      $mid \gets$ \Call{Partition}{$a$, $l$, $r$}\\
      \Call{QuickSort}{$a$, $l$, $mid-1$}\\
      \Call{QuickSort}{$a$, $mid$, $r$}
    }
  }
  \caption{Quick Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N)$ \\
Average case time complexity: $O(N \log N)$\\
Worst case time complexity: $O(N^2)$ \\
Worst case space complexity: $O(1)$

\subsection{Counting Sort}
\subsubsection*{Idea}
Counting sort is a sorting algorithm that sorts the elements of an array by counting the number of occurrences of each unique element in the array. 
The count is stored in an auxiliary array and the sorting is done by mapping the count as an index of the auxiliary array.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  $maxVal \gets a_0$\\
  \For{$i \gets 1$ to $N$}{
    \If{$a_i > maxVal$}{
      $maxVal \gets a_i$
    }
  }
  $count \gets [0]*(maxVal+1)$ \tcp{initialize 0-value counting array}
  \ForEach{$u \in a$}{
    $count_u \gets count_u + 1$
  }
  \tcp{restore the elements to array}
  $idx \gets 0$\\
  \For{$i \gets 0$ to $maxVal$}{
    \While{$count_i > 0$}{
      $a_{idx} \gets i$\\
      $idx \gets idx + 1$\\
      $count_i \gets count_i - 1$
    }
  }
  
  \caption{Counting Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(\text{MaxValue})$ \\
Average case time complexity: $O(\text{MaxValue})$\\
Worst case time complexity: $O(\text{MaxValue})$ \\
Worst case space complexity: $O(\text{MaxValue})$\\
where MaxValue is the maximum value of input array.

\subsection{Radix Sort}
\subsubsection*{Idea}
The idea of Radix Sort is to do digit by digit sort starting from least significant digit to most significant digit. 
Radix sort uses counting sort as a subroutine to sort.

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  $maxVal \gets a_0$\\
  \For{$i \gets 1$ to $N$}{
    \If{$a_i > maxVal$}{
      $maxVal \gets a_i$
    }
  }
  $exp \gets 1$\\
  \While{$\dfrac{maxVal}{exp} > 0$}{
    $digit \gets$ array with of N elements\\
    \For{$i \gets 1$ to $N$}{
      \tcp{get corresponding digit}
      $digit_i \gets \dfrac{a_i}{exp} \mod{10}$\\ 
    }
    \tcp{do counting sort of a[] according to the digit represented by exp}
    \Call{CountingSort}{$a$, $n$, $digit$}\\
    $exp \gets exp \cdot 10$
  }
  
  \caption{Radix Sort}
\end{algorithm2e}

\subsubsection*{Complexity}
Best case time complexity: $O(N \cdot d)$ \\
Average case time complexity: $O(N \cdot d)$\\
Worst case time complexity: $O(N \cdot d)$ \\
Worst case space complexity: $O(N)$\\
where $d$ is the maximum number of digits

\subsection{Flash Sort}
\subsubsection*{Idea}
The main idea of Flash Sort is to assign each of the n input elements to one of m partitions, 
efficiently rearranges the input to place the partitions in the correct order, then sorts each partition.
\newline
\newline
The algorithm can be represented as four stages:
\begin{enumerate}
  \item The number of partitions is calculated.
  \item Set clear boundaries in our original array for every partitions.
  \item Rearrange the elements in the original array so that each of them was in its place, 
  in its partition.
  \item Do Insertion Sort for sorting locally.
\end{enumerate}

\subsubsection*{Pseudo code}
\begin{algorithm2e}
  \KwIn{$a_1, a_2, ..., a_N$}
  \KwOut{$a_1, a_2, ..., a_N$ (in sorted)}
  \SetAlgoLined
  \tcp{stage 1}
  \tcp{d should be in range [0.4, 0.6]}
  $m \gets d \cdot n$\\

  \tcp{stage 2}
  $minVal \gets a_1$\\
  $maxIndex \gets 1$\\
  \For{$i \gets 1$ to $N$}{
    \If{$a_i < minVal$}{
      $minVal \gets a_i$
    }
    \If{$a_{maxIndex} < a_i$}{
      $maxIndex \gets i$
    }
  }
  \If{$a_{maxIndex} == minVal$}{
    stop algorithm
  }
  
  \tcp{classify elements into corresponding partition}
  $c \gets \dfrac{m-1}{a_{maxIndex} - minVal}$\\
  \For{$i \gets 1$ to $N$}{
    $cls \gets c \cdot (a_i - minVal)$\\
    $L_{cls} \gets L_{cls} + 1$ 
  }
  \For{$i \gets 1$ to m}{
    $L_i \gets L_i + L_{i-1}$
  }
  
  \tcp{stage 3}
  swap($a_{maxIndex}, a_1$)
  $nmove \gets 0$\\
  $j \gets 0$\\
  $k \gets m-1$\\
  $t \gets 0$\\
  
  \While{$nmove < N$}{
    \While{$j > L_k - 1$}{
      $j \gets j + 1$\\
      $k \gets c \cdot (a_j - minVal)$
    }
    $flash \gets a_j$\\
    \If{k < 0}{
      \textbf{break}
    }
    \While{$j \neq L_k$}{
      $k \gets c \cdot (flash - minVal)$\\
      $L_k \gets L_k - 1$\\
      $t \gets L_k$\\
      $hold \gets a_t$\\
      $a_t \gets flash$\\
      $flash \gets hold$\\
      $nmove \gets nmove + 1$
    }
    \tcp{stage 4}
    InsertionSort($a$, $n$)
  }
  
  \caption{Flash Sort}
\end{algorithm2e}

\newpage
\subsubsection*{Complexity}
The time complexity of Flash Sort base on choosing value $m$.
For example, if $m$ is chosen proportional to $\sqrt n$, the time complexity is $O(n^{3/2})$.
In this paper, I chose $0.45$ for $m$.
\newline
Space complexity: $O(1)$

\section{Experimental results}
All the eleven sorting algorithms were implemented in C++ programming language and tested on six input of lenght 10000,
30000, 50000, 100000, 300000, and 500000 of four data orders (Sorted data, Nearly sorted data, Reverse sorted data and Randomized data).
All experiments were executed on machine Operating System having Intel(R) Core(TM) i5-10210U CPU @ 1.60Ghz (8 CPUs) and installed memory (RAM) 8GB.
The results were calculated after tabulation and their graphical represention was developed using Python programming language.
\newline

%----------------------------------- Sorted data -------------------------------%

\begin{table}[H]
  \centering
  \small
\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \multicolumn{7}{|c|}{Data Order: Sorted data} \\
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{10,000}} & \multicolumn{2}{|c|}{\text{30,000}} & \multicolumn{2}{|c|}{\text{50,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 112.733 & 50005001 & 993.92 & 450015001 & 2661.613 & 1250025001 \\
  \hline
  Insertion & 0.052 & 19999 & 0.119 & 59999 & 0.177 & 99999 \\
  \hline
  Bubble & 0.029 & 20001 & 0.067 & 60001 & 0.114 & 100001 \\
  \hline
  Shaker & 0.026 & 20001 & 0.073 & 60001 & 0.129 & 100001 \\
  \hline
  Shell & 0.558 & 240024 & 1.762000 & 780029 & 3.423000 & 1400028 \\
  \hline
  Heap & 2.333 & 518705 & 5.418 & 1739633 & 9.129000 & 3056481 \\
  \hline
  Merge & 1.694 & 406234 & 3.632 & 1332186 & 6.21 & 2320874 \\
  \hline
  Quick & 0.598 & 193611 & 1.327 & 627227 & 2.218 & 1084459 \\
  \hline
  Counting & 0.162 & 60003 & 0.369 & 180003 & 0.572 & 300003 \\
  \hline
  Radix & 1.489 & 170106 & 3.738 & 630132 & 7.198 & 1050132 \\
  \hline
  Flash & 0.597 & 103496 & 1.118 & 310496 & 2.359 & 517496 \\
  \hline
\end{tabular}

\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{100,000}} & \multicolumn{2}{|c|}{\text{300,000}} & \multicolumn{2}{|c|}{\text{500,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 11578.892 & 5000050001 & 101293.857 & 45000150001 & 281740.886 & 125000250001 \\
  \hline
  Insertion & 0.411 & 199999 & 1.044 & 599999 & 1.829 & 999999 \\
  \hline
  Bubble & 0.282 & 200001 & 0.765 & 600001 & 1.303 & 1000001  \\
  \hline
  Shaker & 0.245 & 200001 & 0.778 & 600001 & 1.272 & 1000001 \\
  \hline
  Shell & 7.336 & 3000029 & 25.944 & 10200035 & 41.04 & 17000033 \\
  \hline
  Heap & 19.542 & 6519813 & 60.3 & 21431637 & 102.634 & 37116275 \\
  \hline
  Merge & 1.694 & 406234 & 3.632 & 1332186 & 6.21 & 2320874 \\
  \hline
  Quick & 13.286 & 4891754 & 43.119 & 15848682 & 71.659 & 27234634 \\
  \hline
  Counting & 1.267 & 600003 & 3.35 & 1800003 & 5.708 & 3000003 \\
  \hline
  Radix & 13 & 2100132 & 44.787 & 7500158 & 76.2 & 12500158 \\
  \hline
  Flash & 4.02 & 1034996 & 11.634 & 3104996 & 19.422 & 5174996 \\
  \hline
\end{tabular}

\caption{Experimental results on sorted data}
\end{table}

%---------------- Nearly sorted data --------------------%

\begin{table}[H]
  \centering
  \small
\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \multicolumn{7}{|c|}{Data Order: Nearly Sorted data} \\
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{10,000}} & \multicolumn{2}{|c|}{\text{30,000}} & \multicolumn{2}{|c|}{\text{50,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 114.804 & 50005001 & 892.508 & 450015001 & 2815.055 & 1250025001 \\
  \hline
  Insertion & 0.272 & 186007 & 0.533 & 421299 & 1.59 & 792443 \\
  \hline
  Bubble & 138.364 & 95109345 & 825.406 & 773520000 & 2760.609 & 2463630480 \\
  \hline
  Shaker & 0.538 & 195793 & 1.251 & 470236 & 2.147 & 833872 \\
  \hline
  Shell & 0.675 & 288983 & 2.146 & 907030 & 4.659 & 1684266 \\
  \hline
  Heap & 1.407 & 518491 & 5.226 & 1739623 & 11.406 & 3056352 \\
  \hline
  Merge & 1.052 & 421044 & 3.477 & 1381719 & 5.788 & 2407406 \\
  \hline
  Quick & 0.422 & 193651 & 1.343 & 627279 & 2.527 & 1084495 \\
  \hline
  Counting & 0.122 & 60003 & 0.364 & 180003 & 0.519 & 300003 \\
  \hline
  Radix & 1.026 & 170106 & 3.725 & 630132 & 6.265 & 1050132 \\
  \hline
  Flash & 0.455 & 103470 & 1.063 & 310464 & 1.951 & 517470 \\
  \hline
\end{tabular}

\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{100,000}} & \multicolumn{2}{|c|}{\text{300,000}} & \multicolumn{2}{|c|}{\text{500,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 10517.345 & 5000050001 & 98131.833 & 45000150001 & 268308.098 & 125000250001 \\
  \hline
  Insertion & 2.862 & 2143771 & 3.917 & 2784471 & 9.615 & 7687035 \\
  \hline
  Bubble & 10614.255 & 9835488445 & 91618.611 & 80529222960 & 244003.655 & 214177110017  \\
  \hline
  Shaker & 6.193 & 2239517 & 9.282 & 3358098 & 21.366 & 8256564 \\
  \hline
  Shell & 9.193 & 3710600 & 28.038 & 11204764 & 39.989 & 19115054 \\
  \hline
  Heap & 18.311 & 6519703 & 57.692 & 21431472 & 90.107 & 37116054 \\
  \hline
  Merge & 13.929 & 5052616 & 40.513 & 16239347 & 63.774 & 27981864 \\
  \hline
  Quick & 4.53 & 2268955 & 15.264 & 7275735 & 22.691 & 12475755 \\
  \hline
  Counting & 0.962 & 600003 & 3.102 & 1800003 & 4.906 & 3000003 \\
  \hline
  Radix & 12.661 & 2100132 & 46.058 & 7500158 & 65.558 & 12500158 \\
  \hline
  Flash & 3.757 & 1034972 & 10.606 & 3104966 & 17.504 & 5174966 \\
  \hline
\end{tabular}

\caption{Experimental results on nearly sorted data}
\end{table}


%---------------- Reverse sorted data --------------------%

\begin{table}[H]
  \centering
  \small
\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \multicolumn{7}{|c|}{Data Order: Nearly Sorted data} \\
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{10,000}} & \multicolumn{2}{|c|}{\text{30,000}} & \multicolumn{2}{|c|}{\text{50,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 114.984 & 50005001 & 1031.539 & 450015001 & 2892.041 & 1250025001 \\
  \hline
  Insertion & 130.475 & 100009999 & 1284.959 & 900029999 & 3295.286 & 2500049999 \\
  \hline
  Bubble & 313.577 & 100020000 & 2865.468 & 900060000 & 7252.795 & 2500100000 \\
  \hline
  Shaker & 315.843 & 100015000 & 3064.203 & 900045000 & 8452.464 & 2500075000 \\
  \hline
  Shell & 0.796 & 355157 & 2.491 & 1164030 & 6.851 & 2144607 \\
  \hline
  Heap & 1.503 & 476739 & 5.202 & 1622791 & 10.956 & 2848016 \\
  \hline
  Merge & 1.118 & 411833 & 3.672 & 1353961 & 6.06 & 2351433 \\
  \hline
  Quick & 0.586 & 203608 & 1.744 & 657224 & 2.843 & 1134456 \\
  \hline
  Counting & 0.098 & 60003 & 0.307 & 180003 & 0.602 & 300003 \\
  \hline
  Radix & 1.44 & 170106 & 5.409 & 630132 & 7.315 & 1050132 \\
  \hline
  Flash & 0.384 & 86006 & 1.197 & 258006 & 1.951 & 430006 \\
  \hline
\end{tabular}

\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{100,000}} & \multicolumn{2}{|c|}{\text{300,000}} & \multicolumn{2}{|c|}{\text{500,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 10639.041 & 5000050001 & 94140.473 & 45000150001 & 254863.245 & 125000250001 \\
  \hline
  Insertion & 12228.275 & 10000099999 & 104455.571 & 90000299999 & 288450.598 & 250000499999 \\
  \hline
  Bubble & 29184.735 & 10000200000 & 245922.443 & 90000600000 & 678899.144 & 250001000000 \\
  \hline
  Shaker & 30121.16 & 10000150000 & 261173.905 & 90000450000 & 745604.906 & 250000750000 \\
  \hline
  Shell & 9.091 & 4589168 & 31.2 & 14901826 & 49.912 & 25357556 \\
  \hline
  Heap & 18.817 & 6087452 & 58.036 & 20187386 & 95.207 & 35135730 \\
  \hline
  Merge & 13.076 & 4952873 & 40.821 & 16029865 & 71.813 & 27643913 \\
  \hline
  Quick & 4.969 & 2368920 & 15.282 & 7575704 & 27.513 & 12975704 \\
  \hline
  Counting & 0.984 & 600003 & 3.02 & 1800003 & 5.921 & 3000003 \\
  \hline
  Radix & 12.885 & 2100132 & 40.176 & 7500158 & 70.372 & 12500158 \\
  \hline
  Flash & 3.859 & 860006 & 11.693 & 2580006 & 19.106 & 4300006 \\
  \hline
\end{tabular}

\caption{Experimental results on reverse sorted data}
\end{table}

%---------------- Randomized sorted data --------------------%

\begin{table}[H]
  \centering
  \small
\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \multicolumn{7}{|c|}{Data Order: Nearly Sorted data} \\
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{10,000}} & \multicolumn{2}{|c|}{\text{30,000}} & \multicolumn{2}{|c|}{\text{50,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 105.579 & 50005001 & 939.037 & 450015001 & 2628.237 & 1250025001 \\
  \hline
  Insertion & 58.019 & 50154899 & 527.028 & 450626857 & 1472.483 & 1252137825 \\
  \hline
  Bubble & 313.057 & 100014960 & 2911.841 & 900031101 & 8322.806 & 2499993072 \\
  \hline
  Shaker & 228.12 & 66809284 & 2131.684 & 600367899 & 6039.776 & 1666554000 \\
  \hline
  Shell & 1.773 & 509398 & 6.287 & 1854582 & 11.558 & 3789217 \\
  \hline
  Heap & 1.746 & 497238 & 5.839 & 1681366 & 13.595 & 2951638 \\
  \hline
  Merge & 1.593 & 463289 & 5.421 & 1528817 & 9.442 & 2664667 \\
  \hline
  Quick & 1.345 & 286986 & 4.395 & 933275 & 8.178 & 1611537 \\
  \hline
  Counting & 0.172 & 60001 & 0.549 & 180003 & 0.895 & 300001 \\
  \hline
  Radix & 0.919 & 170106 & 3.41 & 630132 & 6.978 & 1050132 \\
  \hline
  Flash & 1.746 & 497238 & 5.839 & 1681366 & 13.595 & 2951638 \\
  \hline
\end{tabular}

\begin{tabular}{ |p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \hline
  \textbf{Data size} & \multicolumn{2}{|c|}{\text{100,000}} & \multicolumn{2}{|c|}{\text{300,000}} & \multicolumn{2}{|c|}{\text{500,000}}\\
  \hline
  \textbf{Result} & Time (ms) & Comparision & Time (ms) & Comparision & Time (ms) & Comparision \\
  \hline
  Selection & 10471.188 & 5000050001 & 96069.631 & 45000150001 & 263963.237 & 125000250001 \\
  \hline
  Insertion & 5785.636 & 4977810633 & 53194.09 & 44944423565 & 147202.734 & 125106226143 \\
  \hline
  Bubble & 33661.256 & 9999933745 & 308169.164 & 89998878657 & 846705.019 & 250000620545 \\
  \hline
  Shaker & 24580.249 & 6650493563 & 228078.787 & 59935751830 & 620102.489 & 166715416590 \\
  \hline
  Shell & 27.963 & 8398740 & 86.959 & 29933626 & 172.871 & 58608607 \\
  \hline
  Heap & 22.234 & 6305394 & 77.598 & 20798645 & 149.053 & 36121064 \\
  \hline
  Merge & 18.848 & 5629341 & 61.769 & 18297818 & 106.604 & 31545308 \\
  \hline
  Quick & 16.023 & 3507410 & 50.365 & 11317513 & 87.032 & 19597319 \\
  \hline
  Counting & 1.88 & 600003 & 5.649 & 1800001 & 18.419 & 3000003 \\
  \hline
  Radix & 10.899 & 2100132 & 42.148 & 7500158 & 65.644 & 12500158 \\
  \hline
  Flash & 4.737 & 726414 & 16.351 & 2174658 & 57.569 & 3681221 \\
  \hline
\end{tabular}

\caption{Experimental results on randomized data}
\end{table}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/sorted_line.png}
  \centering
  \begin{fignote} 
    Note: Running sorting algorithms on sorted input data, almost all algorithms recognized that 
    the data had been sorted except Selection Sort. So in the figure, the line of Selection Sort is significantly higher than the others.
  \end{fignote}
  \caption{Visualizing the algorithms' running times on sorted data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/nsorted_line.png}
  \centering
  \begin{fignote} 
    Note: In the figure, the lines of Selection Sort and Bubble Sort are higher than the others.
    With this data, Selection Sort worked better than Bubble Sort.
  \end{fignote}
  \caption{Visualizing the algorithms' running times on nearly sorted data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/rev_line.png}
  \centering
  \begin{fignote} 
    Note: Shaker and Bubble Sort have the line quite higher when Insertion Sort and Selection Sort are lower. 
    Those show that Insertion and Selection Sort worked better than Shaker and Bubble Sort on reverse sorted data but not good enough 
    when comparing with other algorithms.
  \end{fignote}
  \caption{Visualizing the algorithms' running times on reverse sorted data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/rand_line.png}
  \centering
  \begin{fignote} 
    Note: Bubble and Shaker Sort have the worst running time on randomized data.
    Selection Sort is a little higher than Insertion Sort. 
    Insertion Sort had proofed that it is the most stable algorithm of all simple sorting algorithm. 
    In other that, those advanced sorting algorithms always work well.
  \end{fignote}
  \caption{Visualizing the algorithms' running times on randomized data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/sorted_bar.png}
  \centering
  \begin{fignote} 
    Note: Selection Sort has the most number of comparisons. Bubble, Shaker and Insertion Sort have the smallest comparisions
    since those algorithms can recognize the input data is sorted or not.
  \end{fignote}
  \caption{Visualizing the algorithms' numbers of comparisons on sorted data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/nsorted_bar.png}
  \centering
  \begin{fignote} 
    Note: Counting Sort has the least and Bubble Sort has the most number of comparisons.
    Counting Sort sorts elements by counting the number of occurrences of each unique element in the array, so the 
    number of comparisons is not affected if the order of data changed.
    Opposite that, Selection, Bubble, Insertion Sort ... do have affected if the order changed, 
    easy to recognize if comparing this figure to the above one.
  \end{fignote}
  \caption{Visualizing the algorithms' numbers of comparisons on nearly sorted data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/rev_bar.png}
  \centering
  \begin{fignote} 
    Note: In reverse sorted data, Counting Sort still kept it as the least number of comparisions.
    When Bubble, Selection, Shaker and Insertion Sort have the most number of comparisions.
    This behavior can be explained that those above algorithms are simple, those are not implemented to be able to recognize if the data is reverse sorted or not.
  \end{fignote}
  \caption{Visualizing the algorithms' numbers of comparisons on reverse sorted data}
\end{figure}

\begin{figure}[H]
  \includegraphics[width = 16cm]{plot/rand_bar.png}
  \centering
  \begin{fignote} 
    Note: The behavior of algorithms is the same as reverse sorted data.
  \end{fignote}
  \caption{Visualizing the algorithms' numbers of comparisons on randomized data}
\end{figure}


Overall, the fastest algorithm is Counting Sort, the slowest is Bubble Sort and Selection Sort (Bubble Sort can recognize sorted data but the 
overall Selection Sort has the average time complexity better than Bubble Sort).

\section{Project organization}
C++ programming language was used in sorting algorithms' implementation.
Python programming language and open libraries (Pandas, Matplotlib, Seaborn) were used in processing data and graphical visualizing.
\newline
Source code: \url{https://github.com/huynhtuan17ti/Sorting-Overview}

\section{References}
\begin{enumerate}
  \item \url{https://www.geeksforgeeks.org/} (Explanations and source code of several sorting algorithms)\\
  \item \url{https://www.wikipedia.org/} (Scientific explanations of all sorting algorithms)\\
  \item \url{https://www.researchgate.net/publication/315662067_Sorting_Algorithms_-_A_Comparative_Study}\\
  \item \url{https://www.researchgate.net/publication/259911982_Review_on_Sorting_Algorithms_A_Comparative_Study}\\
  \item Introduction to Algorithms (Third edition)\\
\end{enumerate}

\end{document}